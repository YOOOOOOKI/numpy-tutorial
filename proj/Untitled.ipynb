{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x90 in position 104: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-77c85882ca73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"embed.npy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mp_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"p_vector\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jasonchuzewei\\anaconda3\\envs\\julyedu\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[0m_ZIP_PREFIX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masbytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PK\\x03\\x04'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m         \u001b[0mmagic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m         \u001b[1;31m# If the file size is less than N, we need to make sure not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;31m# to seek past the beginning of the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jasonchuzewei\\anaconda3\\envs\\julyedu\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x90 in position 104: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "embed = np.load(open(\"embed.npy\", \"r\"))\n",
    "p_vector = np.load(open(\"p_vector\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(in_file, relabeling=True):\n",
    "    docs = []\n",
    "    labels = []\n",
    "    num_examples = 0\n",
    "    f = open(in_file, 'r')\n",
    "    line = f.readline()\n",
    "    while line != \"\": \n",
    "        line = line.strip().split(\"\\t\") \n",
    "        \n",
    "        if len(line) >= 2:\n",
    "            docs.append(line[0].split())\n",
    "            labels.append(line[1])\n",
    "            num_examples += 1\n",
    "        else:\n",
    "            docs.append(line[0].split())\n",
    "            num_examples += 1\n",
    "\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "    return (docs, labels)\n",
    "\n",
    "def encode(examples, word_dict, pos_examples=None, pos_dict=None, sort_by_len=True):\n",
    "    '''\n",
    "        Encode the sequences. \n",
    "    '''\n",
    "    in_doc = []\n",
    "    in_l = []\n",
    "    in_pos = []\n",
    "\n",
    "    \n",
    "    if pos_examples is not None:\n",
    "        for idx, (d_words, l_words, pos_words) in enumerate(zip(examples[0], examples[1], pos_examples[0])):\n",
    "            seq1 = [word_dict[w] if w in word_dict else 0 for w in d_words]\n",
    "            seq2 = [int(w) for w in l_words]\n",
    "            seq3 = [pos_dict[w] if w in pos_dict else 0 for w in pos_words]\n",
    "            \n",
    "            if len(seq1) > 0:\n",
    "                in_doc.append(seq1)\n",
    "                in_l.append(seq2)\n",
    "                in_pos.append(seq3)\n",
    "    else:\n",
    "        for idx, (d_words, l_words) in enumerate(zip(examples[0], examples[1])):\n",
    "            seq1 = [word_dict[w] if w in word_dict else 0 for w in d_words]\n",
    "            seq2 = [int(w) for w in l_words]\n",
    "            \n",
    "            if len(seq1) > 0:\n",
    "                in_doc.append(seq1)\n",
    "                in_l.append(seq2)\n",
    "\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "    if sort_by_len:\n",
    "        # sort by the document length\n",
    "        sorted_index = len_argsort(in_doc)\n",
    "        in_doc = [in_doc[i] for i in sorted_index]\n",
    "        in_l = [in_l[i] for i in sorted_index]\n",
    "        if pos_examples is not None:\n",
    "            in_pos = [in_pos[i] for i in sorted_index]\n",
    "\n",
    "    if pos_examples is not None:\n",
    "        return in_doc, in_l, in_pos\n",
    "    else:\n",
    "        return in_doc, in_l\n",
    "\n",
    "def get_minibatches(n, minibatch_size, shuffle=False):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches\n",
    "\n",
    "def prepare_data(seqs):\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    max_len = np.max(lengths)\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    x_mask = np.zeros((n_samples, max_len)).astype('float32')\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "        x_mask[idx, :lengths[idx]] = 1.0\n",
    "    return x, x_mask\n",
    "\n",
    "def gen_examples(d, l, batch_size, pos=None):\n",
    "\n",
    "    minibatches = get_minibatches(len(d), batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_d = [d[t] for t in minibatch]\n",
    "        mb_l = [l[t] for t in minibatch]\n",
    "        mb_d, mb_mask_d = prepare_data(mb_d)\n",
    "        if pos is not None:\n",
    "            mb_pos = [pos[t] for t in minibatch]\n",
    "            mb_pos, mb_mask_pos = prepare_data(mb_pos)\n",
    "            all_ex.append((mb_d, mb_mask_d, mb_l, mb_pos))\n",
    "        else:\n",
    "            all_ex.append((mb_d, mb_mask_d, mb_l))\n",
    "    return all_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_data(\"senti.binary.test.txt\", \"r\")\n",
    "docs, labels = utils.encode(data, word_dict)\n",
    "data = utils.gen_examples(d_docs, d_labels, args.batch_size)\n",
    "for idx, (mb_d, mb_mask_d, mb_l) in enumerate(data):\n",
    "    \n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
